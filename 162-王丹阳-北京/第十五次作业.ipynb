{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1、mtcnn\n",
    "from keras.layers import Conv2D, Input, MaxPool2D, Flatten, Dense, Permute\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import utils\n",
    "import cv2\n",
    "\n",
    "\n",
    "# 获取人脸框,输出bbox位置和是否有人脸\n",
    "def create_Pnet(weight_path):\n",
    "    input = Input(shape=[None, None, 3])\n",
    "\n",
    "    x = Conv2D(10, (3, 3), strides=1, padding='valid', name='conv1')(input)\n",
    "    x = PReLU(shared_axes=[1, 2], name='PReLU1')(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "\n",
    "    x = Conv2D(16, (3, 3), strides=1, padding='valid', name='conv2')(x)\n",
    "    x = PReLU(shared_axes=[1, 2], name='PReLU2')(x)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), strides=1, padding='valid', name='conv3')(x)\n",
    "    x = PReLU(shared_axes=[1, 2], name='PReLU3')(x)\n",
    "\n",
    "    classifier = Conv2D(2, (1, 1), activation='softmax', name='conv4-1')(x)\n",
    "    # 无激活函数，线性\n",
    "    bbox_regress = Conv2D(4, (1, 1), name='conv4-2')(x)\n",
    "    model = Model([input], [classifier, bbox_regress])\n",
    "    model.load_weights(weight_path, by_name=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "#   精确框的位置\n",
    "def create_Rnet(weight_path):\n",
    "    input = Input(shape=[24, 24, 3])\n",
    "\n",
    "    x = Conv2D(28, (3, 3), strides=1, padding='valid', name='conv1')(input)\n",
    "    x = PReLU(shared_axes=[1, 2], name='prelu1')(x)\n",
    "    x = MaxPool2D(pool_size=3, strides=2, padding='same')(x)\n",
    "\n",
    "    x = Conv2D(48, (3, 3), strides=1, padding='valid', name='conv2')(x)\n",
    "    x = PReLU(shared_axes=[1, 2], name='prelu2')(x)\n",
    "    x = MaxPool2D(pool_size=3, strides=2)(x)\n",
    "\n",
    "    x = Conv2D(64, (2, 2), strides=1, padding='valid', name='conv3')(x)\n",
    "    x = PReLU(shared_axes=[1, 2], name='prelu3')(x)\n",
    "\n",
    "    x = Permute((3, 2, 1))(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(128, name='conv4')(x)\n",
    "    x = PReLU(name='prelu4')(x)\n",
    "    classifier = Dense(2, activation='softmax', name='conv5-1')(x)\n",
    "    bbox_regress = Dense(4, name='conv5-2')(x)\n",
    "    model = Model([input], [classifier, bbox_regress])\n",
    "    model.load_weights(weight_path, by_name=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "#   获得框的五个点\n",
    "def create_Onet(weight_path):\n",
    "\n",
    "    input = Input(shape=[48, 48, 3])\n",
    "    x = Conv2D(32, (3, 3), strides=1, padding='valid', name='conv1')(input)\n",
    "    x = PReLU(shared_axes=[1, 2], name='prelu1')(x)\n",
    "    x = MaxPool2D(pool_size=3, strides=2, padding='same')(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), strides=1, padding='valid', name='conv2')(x)\n",
    "    x = PReLU(shared_axes=[1, 2], name='prelu2')(x)\n",
    "    x = MaxPool2D(pool_size=3, strides=2)(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), strides=1, padding='valid', name='conv3')(x)\n",
    "    x = PReLU(shared_axes=[1, 2], name='prelu3')(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "\n",
    "    x = Conv2D(128, (2, 2), strides=1, padding='valid', name='conv4')(x)\n",
    "    x = PReLU(shared_axes=[1, 2], name='prelu4')(x)\n",
    "\n",
    "    x = Permute((3, 2, 1))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, name='conv5')(x)\n",
    "    x = PReLU(name='prelu5')(x)\n",
    "\n",
    "    # 鉴别\n",
    "    classifier = Dense(2, activation='softmax', name='conv6-1')(x)\n",
    "    bbox_regress = Dense(4, name='conv6-2')(x)\n",
    "    landmark_regress = Dense(10, name='conv6-3')(x)\n",
    "\n",
    "    model = Model([input], [classifier, bbox_regress, landmark_regress])\n",
    "    model.load_weights(weight_path, by_name=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class mtcnn():\n",
    "    def __init__(self):\n",
    "        self.Pnet = create_Pnet('../../../../data/mtcnn-keras-master/model_data/pnet.h5')\n",
    "        self.Rnet = create_Rnet('../../../../data/mtcnn-keras-master/model_data/rnet.h5')\n",
    "        self.Onet = create_Onet('../../../../data/mtcnn-keras-master/model_data/onet.h5')\n",
    "\n",
    "    def detectFace(self, img, threshold):\n",
    "\n",
    "        #   归一化\n",
    "        copy_img = (img.copy() - 127.5) / 127.5\n",
    "        origin_h, origin_w, _ = copy_img.shape\n",
    "        # 计算原始输入图像,每一次缩放的比例\n",
    "        scales = utils.calculateScales(img)\n",
    "        out = []\n",
    "\n",
    "        #   粗略计算人脸框\n",
    "        for scale in scales:\n",
    "            hs = int(origin_h * scale)\n",
    "            ws = int(origin_w * scale)\n",
    "            scale_img = cv2.resize(copy_img, (ws, hs))\n",
    "            inputs = scale_img.reshape(1, *scale_img.shape)\n",
    "            # 图像金字塔中的每张图片分别传入Pnet得到output\n",
    "            output = self.Pnet.predict(inputs)\n",
    "            # 将所有output加入out\n",
    "            out.append(output)\n",
    "\n",
    "        image_num = len(scales)\n",
    "        rectangles = []\n",
    "        for i in range(image_num):\n",
    "            # 有人脸的概率\n",
    "            cls_prob = out[i][0][0][:, :, 1]\n",
    "            # 其对应的框的位置\n",
    "            roi = out[i][1][0]\n",
    "            # 取出每个缩放后图片的长宽\n",
    "            out_h, out_w = cls_prob.shape\n",
    "            out_side = max(out_h, out_w)\n",
    "            # print(cls_prob.shape)\n",
    "            # 解码\n",
    "            rectangle = utils.detect_face_12net(cls_prob, roi, out_side, 1 / scales[i], origin_w, origin_h,\n",
    "                                                threshold[0])\n",
    "            rectangles.extend(rectangle)\n",
    "\n",
    "        # 非极大抑制\n",
    "        rectangles = utils.NMS(rectangles, 0.7)\n",
    "        if len(rectangles) == 0:\n",
    "            return rectangles\n",
    "\n",
    "        # 稍微精确计算人脸框\n",
    "        predict_24_batch = []\n",
    "        for rectangle in rectangles:\n",
    "            crop_img = copy_img[int(rectangle[1]):int(rectangle[3]), int(rectangle[0]):int(rectangle[2])]\n",
    "            scale_img = cv2.resize(crop_img, (24, 24))\n",
    "            predict_24_batch.append(scale_img)\n",
    "\n",
    "        predict_24_batch = np.array(predict_24_batch)\n",
    "        out = self.Rnet.predict(predict_24_batch)\n",
    "\n",
    "        cls_prob = out[0]\n",
    "        cls_prob = np.array(cls_prob)\n",
    "        roi_prob = out[1]\n",
    "        roi_prob = np.array(roi_prob)\n",
    "        rectangles = utils.filter_face_24net(cls_prob, roi_prob, rectangles, origin_w, origin_h, threshold[1])\n",
    "\n",
    "        if len(rectangles) == 0:\n",
    "            return rectangles\n",
    "\n",
    "        # 计算人脸框\n",
    "        predict_batch = []\n",
    "        for rectangle in rectangles:\n",
    "            crop_img = copy_img[int(rectangle[1]):int(rectangle[3]), int(rectangle[0]):int(rectangle[2])]\n",
    "            scale_img = cv2.resize(crop_img, (48, 48))\n",
    "            predict_batch.append(scale_img)\n",
    "\n",
    "        predict_batch = np.array(predict_batch)\n",
    "        output = self.Onet.predict(predict_batch)\n",
    "        cls_prob = output[0]\n",
    "        roi_prob = output[1]\n",
    "        pts_prob = output[2]\n",
    "\n",
    "        rectangles = utils.filter_face_48net(cls_prob, roi_prob, pts_prob, rectangles, origin_w, origin_h, threshold[2])\n",
    "\n",
    "        return rectangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2、yolo3\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "class yolo:\n",
    "    def __init__(self, norm_epsilon, norm_decay, anchors_path, classes_path, pre_train):\n",
    "        \"\"\"\n",
    "        初始化函数\n",
    "        norm_decay: 在预测时计算moving average时的衰减率\n",
    "        norm_epsilon: 方差加上极小的数，防止除以0的情况\n",
    "        anchors_path: yolo anchor 文件路径\n",
    "        classes_path: 数据集类别对应文件\n",
    "        pre_train: 是否使用预训练darknet53模型\n",
    "        \"\"\"\n",
    "        self.norm_epsilon = norm_epsilon\n",
    "        self.norm_decay = norm_decay\n",
    "        self.anchors_path = anchors_path\n",
    "        self.classes_path = classes_path\n",
    "        self.pre_train = pre_train\n",
    "        self.anchors = self._get_anchors()\n",
    "        self.classes = self._get_class()\n",
    "\n",
    "    # 获取种类和先验框\n",
    "    def _get_class(self):\n",
    "        \"\"\"\n",
    "        获取类别名字\n",
    "        返回: coco数据集类别对应的名字\n",
    "        \"\"\"\n",
    "        classes_path = os.path.expanduser(self.classes_path)\n",
    "        with open(classes_path) as f:\n",
    "            class_names = f.readlines()\n",
    "        class_names = [c.strip() for c in class_names]\n",
    "        return class_names\n",
    "\n",
    "    def _get_anchors(self):\n",
    "        \"\"\"\n",
    "        获取anchors\n",
    "        \"\"\"\n",
    "        anchors_path = os.path.expanduser(self.anchors_path)\n",
    "        with open(anchors_path) as f:\n",
    "            anchors = f.readline()\n",
    "        anchors = [float(x) for x in anchors.split(',')]\n",
    "        return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "    # l2 正则化\n",
    "    def _batch_normalization_layer(self, input_layer, name = None, training = True, norm_decay = 0.99, norm_epsilon = 1e-3):\n",
    "        \"\"\"\n",
    "        对卷积层提取的feature map使用batch normalization\n",
    "        input_layer: 输入的四维tensor\n",
    "        name: batchnorm层的名字\n",
    "        trainging: 是否为训练过程\n",
    "        norm_decay: 在预测时计算moving average时的衰减率\n",
    "        norm_epsilon: 方差加上极小的数，防止除以0的情况\n",
    "        \"\"\"\n",
    "        bn_layer = tf.layers.batch_normalization(inputs=input_layer, momentum=norm_decay, epsilon=norm_epsilon,\n",
    "                                                 center=True, scale=True, training=training, name=name)\n",
    "        return tf.nn.leaky_relu(bn_layer, alpha=0.1)\n",
    "\n",
    "    # 这个就是用来进行卷积的\n",
    "    def _conv2d_layer(self, inputs, filters_num, kernel_size, name, use_bias = False, strides = 1):\n",
    "        \"\"\"\n",
    "        inputs: 输入变量\n",
    "        filters_num: 卷积核数量\n",
    "        strides: 卷积步长\n",
    "        name: 卷积层名字\n",
    "        trainging: 是否为训练过程\n",
    "        use_bias: 是否使用偏置项\n",
    "        kernel_size: 卷积核大小\n",
    "        \"\"\"\n",
    "        conv = tf.layers.conv2d(\n",
    "            inputs=inputs, filters=filters_num,\n",
    "            kernel_size=kernel_size, strides=[strides, strides], kernel_initializer=tf.glorot_uniform_initializer(),\n",
    "            padding=('SAME' if strides == 1 else 'VALID'), kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=5e-4),\n",
    "            use_bias=use_bias, name=name)\n",
    "        return conv\n",
    "\n",
    "    def _Residual_block(self, inputs, filters_num, blocks_num, conv_index, training=True, norm_decay=0.99, norm_epsilon = 1e-3):\n",
    "        \"\"\"\n",
    "        进行残差卷积的\n",
    "        inputs: 输入变量\n",
    "        filters_num: 卷积核数量\n",
    "        trainging: 是否为训练过程\n",
    "        blocks_num: block的数量\n",
    "        conv_index: 为了方便加载预训练权重，统一命名序号\n",
    "        weights_dict: 加载预训练模型的权重\n",
    "        norm_decay: 在预测时计算moving average时的衰减率\n",
    "        norm_epsilon: 方差加上极小的数，防止除以0的情况\n",
    "        \"\"\"\n",
    "        inputs = tf.pad(inputs, paddings=[[0, 0], [1, 0], [1, 0], [0, 0]], mode='CONSTANT')\n",
    "        layer = self._conv2d_layer(inputs, filters_num, kernel_size=3, strides=2, name=\"conv2d_\" + str(conv_index))\n",
    "        layer = self._batch_normalization_layer(layer, name=\"batch_normalization_\" + str(conv_index), training=training,\n",
    "                                                norm_decay=norm_decay, norm_epsilon=norm_epsilon)\n",
    "        conv_index += 1\n",
    "        for _ in range(blocks_num):\n",
    "            shortcut = layer\n",
    "            layer = self._conv2d_layer(layer, filters_num // 2, kernel_size=1, strides=1, name=\"conv2d_\" + str(conv_index))\n",
    "            layer = self._batch_normalization_layer(layer, name=\"batch_normalization_\" + str(conv_index), training=training,\n",
    "                                                    norm_decay=norm_decay, norm_epsilon=norm_epsilon)\n",
    "            conv_index += 1\n",
    "            layer = self._conv2d_layer(layer, filters_num, kernel_size=3, strides=1, name=\"conv2d_\" + str(conv_index))\n",
    "            layer = self._batch_normalization_layer(layer, name=\"batch_normalization_\" + str(conv_index),\n",
    "                                                    training=training, norm_decay=norm_decay, norm_epsilon=norm_epsilon)\n",
    "            conv_index += 1\n",
    "            layer += shortcut\n",
    "        return layer, conv_index\n",
    "\n",
    "    def _darknet53(self, inputs, conv_index, training = True, norm_decay = 0.99, norm_epsilon = 1e-3):\n",
    "        \"\"\"\n",
    "        构建yolo3使用的darknet53网络结构\n",
    "        inputs: 模型输入变量\n",
    "        conv_index: 卷积层数序号，方便根据名字加载预训练权重\n",
    "        weights_dict: 预训练权重\n",
    "        training: 是否为训练\n",
    "        norm_decay: 在预测时计算moving average时的衰减率\n",
    "        norm_epsilon: 方差加上极小的数，防止除以0的情况\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('darknet53'):\n",
    "            conv = self._conv2d_layer(inputs, filters_num=32, kernel_size=3, strides=1, name=\"conv2d_\" + str(conv_index))\n",
    "            conv = self._batch_normalization_layer(conv, name=\"batch_normalization_\" + str(conv_index),\n",
    "                                                   training=training, norm_decay=norm_decay, norm_epsilon=norm_epsilon)\n",
    "            conv_index += 1\n",
    "            conv, conv_index = self._Residual_block(conv, conv_index=conv_index, filters_num=64, blocks_num=1,\n",
    "                                                    training=training, norm_decay=norm_decay, norm_epsilon=norm_epsilon)\n",
    "            conv, conv_index = self._Residual_block(conv, conv_index=conv_index, filters_num=128, blocks_num=2,\n",
    "                                                    training=training, norm_decay=norm_decay, norm_epsilon=norm_epsilon)\n",
    "            conv, conv_index = self._Residual_block(conv, conv_index=conv_index, filters_num=256, blocks_num=8,\n",
    "                                                    training=training, norm_decay=norm_decay, norm_epsilon=norm_epsilon)\n",
    "            route1 = conv\n",
    "            conv, conv_index = self._Residual_block(conv, conv_index=conv_index, filters_num=512, blocks_num=8,\n",
    "                                                    training=training, norm_decay=norm_decay, norm_epsilon=norm_epsilon)\n",
    "            route2 = conv\n",
    "            conv, conv_index = self._Residual_block(conv, conv_index=conv_index,  filters_num=1024, blocks_num=4,\n",
    "                                                    training=training, norm_decay=norm_decay, norm_epsilon=norm_epsilon)\n",
    "        return route1, route2, conv, conv_index\n",
    "\n",
    "    def _yolo_block(self, inputs, filters_num, out_filters, conv_index, training = True, norm_decay = 0.99, norm_epsilon = 1e-3):\n",
    "        \"\"\"\n",
    "        yolo3在Darknet53提取的特征层基础上，又加了针对3种不同比例的feature map的block，这样来提高对小物体的检测率\n",
    "        inputs: 输入特征\n",
    "        filters_num: 卷积核数量\n",
    "        out_filters: 最后输出层的卷积核数量\n",
    "        conv_index: 卷积层数序号，方便根据名字加载预训练权重\n",
    "        training: 是否为训练\n",
    "        norm_decay: 在预测时计算moving average时的衰减率\n",
    "        norm_epsilon: 方差加上极小的数，防止除以0的情况\n",
    "        \"\"\"\n",
    "        conv = self._conv2d_layer(inputs, filters_num=filters_num, kernel_size=1, strides=1, name=\"conv2d_\" + str(conv_index))\n",
    "        conv = self._batch_normalization_layer(conv, name=\"batch_normalization_\" + str(conv_index), training=training,\n",
    "                                               norm_decay=norm_decay, norm_epsilon=norm_epsilon)\n",
    "        conv_index += 1\n",
    "        conv = self._conv2d_layer(conv, filters_num=filters_num * 2, kernel_size=3, strides=1, name=\"conv2d_\" + str(conv_index))\n",
    "        conv = self._batch_normalization_layer(conv, name=\"batch_normalization_\" + str(conv_index), training=training,\n",
    "                                               norm_decay=norm_decay, norm_epsilon=norm_epsilon)\n",
    "        conv_index += 1\n",
    "        conv = self._conv2d_layer(conv, filters_num=filters_num, kernel_size=1, strides=1, name=\"conv2d_\" + str(conv_index))\n",
    "        conv = self._batch_normalization_layer(conv, name=\"batch_normalization_\" + str(conv_index), training=training,\n",
    "                                               norm_decay=norm_decay, norm_epsilon=norm_epsilon)\n",
    "        conv_index += 1\n",
    "        conv = self._conv2d_layer(conv, filters_num=filters_num * 2, kernel_size=3, strides=1, name=\"conv2d_\" + str(conv_index))\n",
    "        conv = self._batch_normalization_layer(conv, name=\"batch_normalization_\" + str(conv_index), training=training,\n",
    "                                               norm_decay=norm_decay, norm_epsilon=norm_epsilon)\n",
    "        conv_index += 1\n",
    "        conv = self._conv2d_layer(conv, filters_num=filters_num, kernel_size=1, strides=1, name=\"conv2d_\" + str(conv_index))\n",
    "        conv = self._batch_normalization_layer(conv, name=\"batch_normalization_\" + str(conv_index), training=training,\n",
    "                                               norm_decay=norm_decay, norm_epsilon=norm_epsilon)\n",
    "        conv_index += 1\n",
    "        route = conv\n",
    "        conv = self._conv2d_layer(conv, filters_num=filters_num * 2, kernel_size=3, strides=1, name=\"conv2d_\" + str(conv_index))\n",
    "        conv = self._batch_normalization_layer(conv, name=\"batch_normalization_\" + str(conv_index), training=training,\n",
    "                                               norm_decay=norm_decay, norm_epsilon=norm_epsilon)\n",
    "        conv_index += 1\n",
    "        conv = self._conv2d_layer(conv, filters_num=out_filters, kernel_size=1, strides=1, name=\"conv2d_\" + str(conv_index),\n",
    "                                  use_bias=True)\n",
    "        conv_index += 1\n",
    "        return route, conv, conv_index\n",
    "\n",
    "    #\n",
    "    def yolo_inference(self, inputs, num_anchors, num_classes, training = True):\n",
    "        \"\"\"\n",
    "        返回三个特征层的内容\n",
    "        inputs: 模型的输入变量\n",
    "        num_anchors: 每个grid cell负责检测的anchor数量\n",
    "        num_classes: 类别数量\n",
    "        training: 是否为训练模式\n",
    "        \"\"\"\n",
    "        conv_index = 1\n",
    "        conv2d_26, conv2d_43, conv, conv_index = self._darknet53(inputs, conv_index, training=training, norm_decay=self.norm_decay, norm_epsilon=self.norm_epsilon)\n",
    "        with tf.variable_scope('yolo'):\n",
    "            conv2d_57, conv2d_59, conv_index = self._yolo_block(conv, 512, num_anchors * (num_classes + 5),\n",
    "                                                                conv_index=conv_index, training=training, norm_decay=self.norm_decay,\n",
    "                                                                norm_epsilon=self.norm_epsilon)\n",
    "\n",
    "            conv2d_60 = self._conv2d_layer(conv2d_57, filters_num=256, kernel_size=1, strides=1, name=\"conv2d_\" + str(conv_index))\n",
    "            conv2d_60 = self._batch_normalization_layer(conv2d_60, name=\"batch_normalization_\" + str(conv_index),\n",
    "                                                        training=training, norm_decay=self.norm_decay, norm_epsilon=self.norm_epsilon)\n",
    "            conv_index += 1\n",
    "            unSample_0 = tf.image.resize_nearest_neighbor(conv2d_60, [2 * tf.shape(conv2d_60)[1], 2 * tf.shape(conv2d_60)[1]],\n",
    "                                                          name='upSample_0')\n",
    "            route0 = tf.concat([unSample_0, conv2d_43], axis=-1, name='route_0')\n",
    "            conv2d_65, conv2d_67, conv_index = self._yolo_block(route0, 256, num_anchors * (num_classes + 5),\n",
    "                                                                conv_index=conv_index, training=training, norm_decay=self.norm_decay, norm_epsilon=self.norm_epsilon)\n",
    "\n",
    "            conv2d_68 = self._conv2d_layer(conv2d_65, filters_num=128, kernel_size=1, strides=1, name=\"conv2d_\" + str(conv_index))\n",
    "            conv2d_68 = self._batch_normalization_layer(conv2d_68, name=\"batch_normalization_\" + str(conv_index),\n",
    "                                                        training=training, norm_decay=self.norm_decay, norm_epsilon=self.norm_epsilon)\n",
    "            conv_index += 1\n",
    "            unSample_1 = tf.image.resize_nearest_neighbor(conv2d_68, [2 * tf.shape(conv2d_68)[1], 2 * tf.shape(conv2d_68)[1]], name='upSample_1')\n",
    "            route1 = tf.concat([unSample_1, conv2d_26], axis=-1, name='route_1')\n",
    "            _, conv2d_75, _ = self._yolo_block(route1, 128, num_anchors * (num_classes + 5), conv_index=conv_index,\n",
    "                                               training=training, norm_decay=self.norm_decay, norm_epsilon=self.norm_epsilon)\n",
    "\n",
    "        return [conv2d_59, conv2d_67, conv2d_75]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
